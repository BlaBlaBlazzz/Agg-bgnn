{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def construct_graph(data):\n",
    "    G = nx.Graph()\n",
    "    nodes = list(data.index)\n",
    "    G.add_nodes_from(nodes)\n",
    "\n",
    "    # print(G.nodes()[1]['features'])\n",
    "    simul = {i:{} for i in range(data.shape[0])}\n",
    "    for id1 in range(data.shape[0]):\n",
    "        for id2 in range(data.shape[0]):\n",
    "            if id1 != id2:\n",
    "                feature1 = data.iloc[id1]\n",
    "                feature2 = data.iloc[id2]\n",
    "\n",
    "                # simularity\n",
    "                simularity = cosine_similarity([feature1], [feature2])[0][0]\n",
    "                simul[id1][id2] = simularity\n",
    "    \n",
    "    # sorted\n",
    "    sorted_simul = {}\n",
    "    \n",
    "    for key, values in simul.items():\n",
    "        sorted_simul[key] = dict(sorted(values.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "        # top 5 simularity\n",
    "        top5 = list(sorted_simul[key].keys())[:5]\n",
    "        for node in top5:\n",
    "            G.add_edge(key, node)\n",
    "            \n",
    "    '''\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=500)\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    # write graphml file\n",
    "    nx.write_graphml(G, \"mygraph.graphml\")\n",
    "\n",
    "def construct_graph2(data, file_name):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "    nodes = list(data.index)\n",
    "\n",
    "    G.add_nodes_from(nodes)\n",
    "    simularity = cosine_similarity(data.values, data.values)\n",
    "    \n",
    "    # sorted_simul = []\n",
    "    for node in nodes:\n",
    "        # reversed argsort\n",
    "        sorted_simul_arg = np.argsort(simularity[node])[::-1]\n",
    "        # exclude itself\n",
    "        top5 = sorted_simul_arg[1:6]\n",
    "        G.add_edges_from([node, n] for n in top5)\n",
    "    \n",
    "    nx.write_graphml(G, file_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "import json\n",
    "\n",
    "def feature_vector(X, y, path):\n",
    "    model = CatBoostClassifier(iterations=100,\n",
    "                                  depth=6,\n",
    "                                  learning_rate=0.1,\n",
    "                                  loss_function='MultiClass',\n",
    "                                  random_seed=0,\n",
    "                                  nan_mode='Min',\n",
    "                                  allow_const_label=True)\n",
    "    \n",
    "    # extract train masks\n",
    "    with open(f'{path}/masks.json') as f:\n",
    "        masks = json.load(f)\n",
    "    train_masks = masks['0']['train']\n",
    "    \n",
    "    X_train = X.iloc[train_masks]\n",
    "    y_train = y.iloc[train_masks]\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    prediction = model.predict_proba(X)\n",
    "    # pred = model.predict(X)\n",
    "    # print((y == pred.max(1)).sum().item()/y.shape[0])\n",
    "    leaf_index = model.calc_leaf_indexes(X)\n",
    "    return prediction, leaf_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def gen_masks(data, path):\n",
    "    masks = {str(i):{\"train\":[], \"val\":[], \"test\":[]} for i in range(5)}\n",
    "    scale_config = [int(0.6*len(data)), int(0.2*len(data)), int(0.2*len(data))]\n",
    "    # print(scale_config)\n",
    "\n",
    "    for key in list(masks.keys()):\n",
    "        ids = list(data.index)\n",
    "        random.shuffle(ids)\n",
    "\n",
    "        masks[key][\"train\"] = ids[: scale_config[0]]\n",
    "        masks[key][\"val\"] = ids[scale_config[0]:scale_config[0]+scale_config[1]]\n",
    "        masks[key][\"test\"] = ids[scale_config[0]+scale_config[1]:]\n",
    "\n",
    "    write_json(masks, path)\n",
    "\n",
    "\n",
    "def write_json(data, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 2701)\n",
      "(60, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path1 = 'slap/X.csv'\n",
    "path2 = 'slap/y.csv'\n",
    "\n",
    "data = pd.read_csv(path1, header=None)[1:]\n",
    "label = pd.read_csv(path2, header=None)[1:]\n",
    "\n",
    "prediction, leaf_index = feature_vector(data, label, 'slap_s4')\n",
    "construct_graph2(prediction, \"pred_graph.graphml\")\n",
    "construct_graph2(leaf_index, \"leaf_graph.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical recognition of handwritten digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5620, 64)\n",
      "(3372, 64)\n",
      "(3372,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path1 = 'optdigit/train_data.csv'\n",
    "path2 = 'optdigit/test_data.csv'\n",
    "\n",
    "train_data = pd.read_csv(path1, header=None)\n",
    "test_data = pd.read_csv(path2, header=None)\n",
    "\n",
    "data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
    "label = data.iloc[:, -1]\n",
    "data = data.iloc[:, :-1]\n",
    "print(data.shape)\n",
    "\n",
    "# create X.csv\n",
    "# data.to_csv(\"X.csv\", index=False)\n",
    "\n",
    "# create y.csv\n",
    "# label.to_csv(\"y.csv\", index=False, header=[\"class\"])\n",
    "\n",
    "# create graph.graphml\n",
    "# construct_graph2(data)\n",
    "\n",
    "# create masks.json\n",
    "# gen_masks(data, \"masks.json\")\n",
    "prediction, leaf_index = feature_vector(data, label, \"optdigit\")\n",
    "# construct_graph2(prediction, \"pred_graph.graphml\")\n",
    "# construct_graph2(leaf_index, \"leaf_graph.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine recognition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 13)\n",
      "(12,)\n",
      "0.9382022471910112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "path = 'wine/wine.data.csv'\n",
    "\n",
    "data = pd.read_csv(path, header=None)\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "label = data.iloc[:, 0]-1\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "# create X.csv\n",
    "# data.to_csv(\"X.csv\", index=False)\n",
    "\n",
    "# create y.csv\n",
    "# label.to_csv(\"y.csv\", index=False, header=[\"class\"])\n",
    "\n",
    "# create graph.graphml\n",
    "# construct_graph2(data)\n",
    "\n",
    "# create masks.json\n",
    "# gen_masks(data, \"masks.json\")\n",
    "\n",
    "# create feature vector\n",
    "prediction, leaf_index = feature_vector(data, label, 'wine_s4')\n",
    "# print(prediction.shape)\n",
    "# print(leaf_index.shape)\n",
    "# construct_graph2(prediction, \"pred_graph.graphml\")\n",
    "# construct_graph2(leaf_index, \"leaf_graph.graphml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer wisconsin (diagnostic) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'wdbc/wdbc.data.csv'\n",
    "\n",
    "data = pd.read_csv(path, header=None)\n",
    "label = data.iloc[:, 1].replace({\"B\":1, \"M\":0})\n",
    "data = data.iloc[:, 2:]\n",
    "\n",
    "# create X.csv\n",
    "data.to_csv(\"X.csv\", index=False)\n",
    "# create y.csv\n",
    "label.to_csv(\"y.csv\", index=False, header=[\"class\"])\n",
    "# create graph.graphml\n",
    "construct_graph2(data)\n",
    "# create masks.json\n",
    "gen_masks(data, \"masks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
