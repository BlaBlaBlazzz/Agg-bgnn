hp:
  layers:
  - 1
  - 2
  lr:
  - 0.0003
  - 0.0001
  - 0.00003
  col_lr:
  - 0.0005
  - 0.001
  - 0.005
  attention_dropout:
  - 0.0
  - 0.3
  - 0.5

eval_batch_size: 8192
num_epochs: 100
patience: 20
